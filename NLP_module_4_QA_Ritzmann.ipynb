{"cells":[{"cell_type":"markdown","metadata":{"id":"iELa7oZ444SV"},"source":["# Installs, Imports & Mounts"]},{"cell_type":"markdown","metadata":{"id":"7b3TpWNZqBX8"},"source":["## Installs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mw50AomdHYyz"},"outputs":[],"source":["%%capture\n","# needed for inferece\n","# install packages\n","# !pip install -q condacolab\n","# !pip install transformers\n","# !pip install datasets\n","# !pip install tokenizersreft\n","!pip install transformers\n","!pip install datasets\n","!pip install tokenizers\n","!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n","!pip install unsloth_zoo\n","!pip install --no-deps packaging ninja einops \"flash-attn>=2.6.3\"\n","!pip install datasets\n","!pip install -U bitsandbytes accelerate\n","!pip install rouge-score==0.1.2\n","!pip install xformers"]},{"cell_type":"markdown","metadata":{"id":"8zmhF7Y5pxa2"},"source":["## Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Q1UEGkDRrGr"},"outputs":[],"source":["# mount drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"2PD6xWcip1Sk"},"source":["## Access tokens"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"dLwWezsFBZys","executionInfo":{"status":"ok","timestamp":1738226315454,"user_tz":-60,"elapsed":1388,"user":{"displayName":"Bryce Maynard","userId":"16855679042583479157"}}},"outputs":[],"source":["# instantiate HF access token\n","# needed for inference\n","from google.colab import userdata\n","HUGGINGFACE_TOKEN = userdata.get('huggingface')"]},{"cell_type":"markdown","metadata":{"id":"c14nWCUDp6w0"},"source":["## Github"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"TtN062hq4i07","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738226318998,"user_tz":-60,"elapsed":3548,"user":{"displayName":"Bryce Maynard","userId":"16855679042583479157"}},"outputId":"f4f85778-6bdb-42e8-ceed-0beb475bddcb"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/git\n","fatal: destination path 'nlp_module_4_QA' already exists and is not an empty directory.\n","/content/drive/MyDrive/git/nlp_module_4_QA\n"]}],"source":["# connect github repo\n","%cd /content/drive/MyDrive/git\n","\n","GITHUB_EMAIL = userdata.get('github_email')\n","GITHUB_TOKEN = userdata.get('github_pat')\n","GITHUB_USERNAME = \"CurseARealSword\"\n","GITHUB_REPO = \"nlp_module_4_QA\"\n","\n","\n","!git config --global user.email \"{GITHUB_EMAIL}\"\n","!git config --global user.name \"CurseARealSword\"\n","\n","\n","\n","!git clone https://github.com/CurseARealSword/nlp_module_4_QA.git\n","%cd /content/drive/MyDrive/git/{GITHUB_REPO}\n","!git remote set-url origin https://{GITHUB_USERNAME}:{GITHUB_TOKEN}@github.com/{GITHUB_USERNAME}/{GITHUB_REPO}.git\n","\n"]},{"cell_type":"code","source":["# !git add /content/drive/MyDrive/git/{GITHUB_REPO}/.gitignore\n","!git pull origin main"],"metadata":{"id":"248BixD-CKfc","executionInfo":{"status":"ok","timestamp":1738226934845,"user_tz":-60,"elapsed":2652,"user":{"displayName":"Bryce Maynard","userId":"16855679042583479157"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"33d8600a-5cc8-4a66-ce2a-fdeec9c72be1"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["remote: Enumerating objects: 4, done.\u001b[K\n","remote: Counting objects:  25% (1/4)\u001b[K\rremote: Counting objects:  50% (2/4)\u001b[K\rremote: Counting objects:  75% (3/4)\u001b[K\rremote: Counting objects: 100% (4/4)\u001b[K\rremote: Counting objects: 100% (4/4), done.\u001b[K\n","remote: Compressing objects:  50% (1/2)\u001b[K\rremote: Compressing objects: 100% (2/2)\u001b[K\rremote: Compressing objects: 100% (2/2), done.\u001b[K\n","remote: Total 3 (delta 1), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n","Unpacking objects:  33% (1/3)\rUnpacking objects:  66% (2/3)\rUnpacking objects: 100% (3/3)\rUnpacking objects: 100% (3/3), 964 bytes | 21.00 KiB/s, done.\n","From https://github.com/CurseARealSword/nlp_module_4_QA\n"," * branch            main       -> FETCH_HEAD\n","   7b9a936..73cff22  main       -> origin/main\n","Auto-merging .gitignore\n","CONFLICT (add/add): Merge conflict in .gitignore\n","Automatic merge failed; fix conflicts and then commit the result.\n"]}]},{"cell_type":"code","execution_count":15,"metadata":{"id":"_1osZttKMLTb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738227219143,"user_tz":-60,"elapsed":149462,"user":{"displayName":"Bryce Maynard","userId":"16855679042583479157"}},"outputId":"d976b074-8340-48f2-a58a-9a0beda332d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Enter commit message: resolve merge\n","[main 63de984] resolve merge\n"," 16 files changed, 841006 insertions(+), 1 deletion(-)\n"," rewrite NLP_module_4_QA_Ritzmann.ipynb (80%)\n"," create mode 100644 huggingface_tokenizers_cache/.locks/models--unsloth--gemma-2-2b-bnb-4bit/3f289bc05132635a8bc7aca7aa21255efd5e18f3710f43e3cdb96bcd41be4922.lock\n"," create mode 100644 huggingface_tokenizers_cache/.locks/models--unsloth--gemma-2-2b-bnb-4bit/61a7b147390c64585d6c3543dd6fc636906c9af3865a5548f27f31aee1d4c8e2.lock\n"," create mode 100644 huggingface_tokenizers_cache/.locks/models--unsloth--gemma-2-2b-bnb-4bit/8d6368f7e735fbe4781bf6e956b7c6ad0586df80.lock\n"," create mode 100644 huggingface_tokenizers_cache/.locks/models--unsloth--gemma-2-2b-bnb-4bit/980936b61554cc3bc64942ab256b038e3fb4ea56.lock\n"," create mode 100644 huggingface_tokenizers_cache/models--unsloth--gemma-2-2b-bnb-4bit/.no_exist/341a487bce54fbeb136e13786461a1e66ec3ffbc/added_tokens.json\n"," create mode 100644 huggingface_tokenizers_cache/models--unsloth--gemma-2-2b-bnb-4bit/.no_exist/341a487bce54fbeb136e13786461a1e66ec3ffbc/chat_template.jinja\n"," create mode 100644 huggingface_tokenizers_cache/models--unsloth--gemma-2-2b-bnb-4bit/blobs/3f289bc05132635a8bc7aca7aa21255efd5e18f3710f43e3cdb96bcd41be4922\n"," create mode 100644 huggingface_tokenizers_cache/models--unsloth--gemma-2-2b-bnb-4bit/blobs/61a7b147390c64585d6c3543dd6fc636906c9af3865a5548f27f31aee1d4c8e2\n"," create mode 100644 huggingface_tokenizers_cache/models--unsloth--gemma-2-2b-bnb-4bit/blobs/8d6368f7e735fbe4781bf6e956b7c6ad0586df80\n"," create mode 100644 huggingface_tokenizers_cache/models--unsloth--gemma-2-2b-bnb-4bit/blobs/980936b61554cc3bc64942ab256b038e3fb4ea56\n"," create mode 100644 huggingface_tokenizers_cache/models--unsloth--gemma-2-2b-bnb-4bit/refs/main\n"," create mode 120000 huggingface_tokenizers_cache/models--unsloth--gemma-2-2b-bnb-4bit/snapshots/341a487bce54fbeb136e13786461a1e66ec3ffbc/special_tokens_map.json\n"," create mode 120000 huggingface_tokenizers_cache/models--unsloth--gemma-2-2b-bnb-4bit/snapshots/341a487bce54fbeb136e13786461a1e66ec3ffbc/tokenizer.json\n"," create mode 120000 huggingface_tokenizers_cache/models--unsloth--gemma-2-2b-bnb-4bit/snapshots/341a487bce54fbeb136e13786461a1e66ec3ffbc/tokenizer.model\n"," create mode 120000 huggingface_tokenizers_cache/models--unsloth--gemma-2-2b-bnb-4bit/snapshots/341a487bce54fbeb136e13786461a1e66ec3ffbc/tokenizer_config.json\n","Enumerating objects: 95, done.\n","Counting objects: 100% (92/92), done.\n","Delta compression using up to 2 threads\n","Compressing objects: 100% (78/78), done.\n","Writing objects: 100% (80/80), 653.60 MiB | 9.63 MiB/s, done.\n","Total 80 (delta 42), reused 1 (delta 0), pack-reused 0\n","remote: Resolving deltas: 100% (42/42), completed with 5 local objects.\u001b[K\n","remote: \u001b[1;33mwarning\u001b[m: File outputs/checkpoint-100/adapter_model.safetensors is 79.26 MB; this is larger than GitHub's recommended maximum file size of 50.00 MB\u001b[K\n","remote: \u001b[1;33mwarning\u001b[m: File outputs/checkpoint-60/adapter_model.safetensors is 79.26 MB; this is larger than GitHub's recommended maximum file size of 50.00 MB\u001b[K\n","remote: \u001b[1;31merror\u001b[m: Trace: 058ea1ee7d2bb1978162389e9837b4731fd9f2df2bfff0ea9a42fb4c14f5fccf\u001b[K\n","remote: \u001b[1;31merror\u001b[m: See https://gh.io/lfs for more information.\u001b[K\n","remote: \u001b[1;31merror\u001b[m: File outputs/checkpoint-120/adapter_model.safetensors is 316.92 MB; this exceeds GitHub's file size limit of 100.00 MB\u001b[K\n","remote: \u001b[1;31merror\u001b[m: File outputs/checkpoint-120/optimizer.pt is 161.32 MB; this exceeds GitHub's file size limit of 100.00 MB\u001b[K\n","remote: \u001b[1;31merror\u001b[m: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com.\u001b[K\n","To https://github.com/CurseARealSword/nlp_module_4_QA.git\n"," \u001b[31m! [remote rejected]\u001b[m main -> main (pre-receive hook declined)\n","\u001b[31merror: failed to push some refs to 'https://github.com/CurseARealSword/nlp_module_4_QA.git'\n","\u001b[m"]}],"source":["# commit changes to github\n","# fetch current notebook file and add it to repo folder\n","!cd /content/drive/MyDrive/git\n","!cp '/content/drive/MyDrive/Colab Notebooks/NLP_module_4_QA_Ritzmann.ipynb' /content/drive/MyDrive/git/nlp_module_4_QA/\n","!git add /content/drive/MyDrive/git/{GITHUB_REPO}/.\n","# prompt user for commit message\n","GITHUB_COMMIT_MESSAGE = input(\"Enter commit message: \")\n","!git commit -m \"{GITHUB_COMMIT_MESSAGE}\"\n","!git push origin main\n"]},{"cell_type":"code","source":["#!git ls-tree -r main --name-only\n","# !git commit -m \"resolve pull conflict\"\n","git fetch origin\n","    git reset --hard origin/main"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y4S6quqIY3zt","executionInfo":{"status":"ok","timestamp":1738227055200,"user_tz":-60,"elapsed":1234,"user":{"displayName":"Bryce Maynard","userId":"16855679042583479157"}},"outputId":"d3a4f223-7e74-492e-ea1c-032d9e8ad64f"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[main 7b0d0ae] resolve pull conflict\n"]}]},{"cell_type":"code","source":["# remove file from git main branch\n","# !git rm /content/drive/MyDrive/git/{GITHUB_REPO}/.gitignore\n","!git pull origin main"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5fmtvwFIZmSb","executionInfo":{"status":"ok","timestamp":1738227062547,"user_tz":-60,"elapsed":1043,"user":{"displayName":"Bryce Maynard","userId":"16855679042583479157"}},"outputId":"366348fc-d7a7-47e1-8949-e7c25da44fb9"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["From https://github.com/CurseARealSword/nlp_module_4_QA\n"," * branch            main       -> FETCH_HEAD\n","Already up to date.\n"]}]},{"cell_type":"markdown","metadata":{"id":"sHVAqJtL-4qV"},"source":["# Baseline Inference Test & Model Saving"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-7B4rNyiAZ52"},"outputs":[],"source":["#%%capture\n","!pip install -U bitsandbytes accelerate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OkaBdgl7-7IS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738054226432,"user_tz":-60,"elapsed":23839,"user":{"displayName":"Bryce Maynard","userId":"16855679042583479157"}},"outputId":"275c3772-3436-498e-ec9c-6b3abd9abc91"},"outputs":[{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth 2025.1.7: Fast Gemma2 patching. Transformers: 4.47.1.\n","   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"stream","name":"stderr","text":["Unsloth 2025.1.7 patched 26 layers with 26 QKV layers, 26 O layers and 26 MLP layers.\n"]}],"source":["from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","\n","\n","quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n","\n","# tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/models/gemma-2-2b\", use_auth_token=HUGGINGFACE_TOKEN)\n","# model = AutoModelForCausalLM.from_pretrained(\n","#     \"/content/drive/MyDrive/models/gemma-2-2b\",\n","#     max_seq_length=2048,\n","#     quantization_config=quantization_config,\n","#     use_auth_token=HUGGINGFACE_TOKEN,\n","# )\n","\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = \"/content/drive/MyDrive/models/LoRA/gemma-2-2b_4bit_LoRA_v4\",\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n","    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",")"]},{"cell_type":"code","source":[],"metadata":{"id":"XqVelXFJAE34"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Eld6WTomQA60"},"outputs":[],"source":["# move the model to the GPU\n","model.to(\"cuda\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-2kOA3oSFTVs"},"outputs":[],"source":["# save loaded model to google drive\n","model_save_path = '/content/drive/MyDrive/models/gemma-2-2b'\n","tokenizer.save_pretrained(model_save_path)\n","model.save_pretrained(model_save_path)\n"]},{"cell_type":"code","source":["# test inference\n","test_prompt = \"You are playing Russian roulette with a six-shooter revolver. Your opponent puts in five bullets, spins the chambers and fires at himself, but no bullet comes out. He gives you the choice of whether or not he should spin the chambers again before firing at you. Should he spin again?\"\n","alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","{}\n","\n","### Input:\n","{}\n","\n","### Response:\n","{}\"\"\"\n","\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","inputs = tokenizer(\n","[\n","    alpaca_prompt.format(#        \"You are playing Russian roulette with a six-shooter revolver. Your opponent puts in five bullets, spins the chambers and fires at himself, but no bullet comes out. He gives you the choice of whether or not he should spin the chambers again before firing at you. Should he spin again?\", # instruction\n","        \"\", # input\n","        \"\", # output - leave this blank for generation!\n","      test_prompt\n",")\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","outputs = model.generate(**inputs, max_new_tokens = 1024, use_cache = True)\n","tokenizer.batch_decode(outputs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gaUB4pixAs3F","executionInfo":{"status":"ok","timestamp":1738054315347,"user_tz":-60,"elapsed":43883,"user":{"displayName":"Bryce Maynard","userId":"16855679042583479157"}},"outputId":"f70b7db8-6e76-4608-9d50-e41c484e21fe"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<bos>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n\\n\\n### Input:\\n\\n\\n### Response:\\nYou are playing Russian roulette with a six-shooter revolver. Your opponent puts in five bullets, spins the chambers and fires at himself, but no bullet comes out. He gives you the choice of whether or not he should spin the chambers again before firing at you. Should he spin again? Why or why not?\\n\\n### Thought Process:\\n1. **Analyze the Problem Requirements**:\\n   - The problem involves a dangerous situation where a person is playing Russian roulette.\\n   - The key factor is whether or not the opponent should spin the chambers again before firing at himself.\\n   - We need to consider the implications of spinning the chambers and the potential outcomes.\\n\\n2. **List the Steps to Solve the Problem**:\\n   - Step 1: Understand the context of Russian roulette.\\n   - Step 2: Analyze the odds of spinning the chambers and the consequences of each outcome.\\n   - Step 3: Evaluate the implications of spinning again versus not spinning again.\\n   - Step 4: Formulate a conclusion based on the analysis.\\n\\n3. **Execute the Solution Process**:\\n   - **Step 1: Understand the Context**:\\n     - Russian roulette is a dangerous game where a person fires a gun at themselves.\\n     - The odds of being shot are significantly higher if the opponent does not spin the chambers.\\n\\n   - **Step 2: Analyze the Odds**:\\n     - If the opponent spins the chambers, there is a chance that he will not fire at himself.\\n     - If he does not spin the chambers, there is a chance that he will fire at himself.\\n     - The odds of being shot are higher if he does not spin the chambers.\\n\\n   - **Step 3: Evaluate the Implications**:\\n     - If he spins the chambers, he has a chance to avoid being shot.\\n     - If he does not spin the chambers, he has a higher chance of being shot.\\n     - The decision of whether or not to spin the chambers is a critical one.\\n\\n   - **Step 4: Formulate a Conclusion**:\\n     - Based on the analysis, it is clear that spinning the chambers is a risky decision.\\n     - If he spins the chambers, he has a chance to avoid being shot, but it is not guaranteed.\\n     - If he does not spin the chambers, he has a higher chance of being shot, but it is not guaranteed.\\n     - The decision of whether or not to spin the chambers is a personal one, and there is no right or wrong answer.\\n\\n### Final Answer:\\nShould he spin again? No, he should not spin again. The odds of being shot are higher if he does not spin the chambers. The decision of whether or not to spin the chambers is a personal one, and there is no right or wrong answer. However, from a purely statistical standpoint, spinning the chambers is a risky decision, and the odds of being shot are higher if he does not spin the chambers.<eos>']"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["# human eval v2\n","\n","FastLanguageModel.for_inference(model)\n","\n","import json\n","\n","with open(\"/content/drive/MyDrive/git/human_eval_questions.json\", \"r\") as f:\n","    questions = json.load(f)\n","\n","import csv\n","\n","qa_pairs = []\n","for question_data in questions:\n","    question = question_data[\"instruction\"]\n","\n","    inputs = tokenizer(question, return_tensors=\"pt\").to(\"cuda\")\n","    outputs = model.generate(**inputs, max_new_tokens=512)\n","    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","    qa_pairs.append([question, answer])\n","\n","with open(\"/content/drive/MyDrive/git/human_eval_answers_gemma_v2.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n","    writer = csv.writer(csvfile)\n","    writer.writerow([\"Question\", \"Answer\"])\n","    writer.writerows(qa_pairs)\n"],"metadata":{"id":"gbSuXWqcBuge"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lphn3sP7UOT8"},"outputs":[],"source":["#%%capture\n","\n","!pip install transformers\n","!pip install datasets\n","!pip install tokenizers\n","!pip install -U bitsandbytes accelerate\n","!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n","!pip install unsloth_zoo\n","\n","# Install Flash Attention 2 for softcapping support\n","import torch\n","if torch.cuda.get_device_capability()[0] >= 8:\n","    !pip install --no-deps packaging ninja einops \"flash-attn>=2.6.3\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FWHPBU7OX7bE"},"outputs":[],"source":["alpaca_prompt = \"You are a helpful assistant!\"\n","# add an EOS token so the gen doesn't go on forever\n","EOS_TOKEN = tokenizer.eos_token\n","\n","FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","inputs = tokenizer(\n","[\n","    \"Write me a poem about Machine Learning.\"\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"]},{"cell_type":"markdown","metadata":{"id":"kQORzf4vXn3p"},"source":["# Dataset generation using CAMEL AI"]},{"cell_type":"markdown","metadata":{"id":"TyCY5waMOEZc"},"source":["## installs and imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FIwIlVFFYDCG"},"outputs":[],"source":["%%capture\n","# install camel\n","!pip install camel-ai==0.2.16\n","!pip install rouge-score==0.1.2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BM2bjZy0dKyE"},"outputs":[],"source":["import os\n","from datetime import datetime\n","import json\n","from camel.datagen.cotdatagen import CoTDataGenerator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qGGv43ftYj1-"},"outputs":[],"source":["# instantiate openai access token\n","from google.colab import userdata\n","openai_api_key = userdata.get('openai')\n","os.environ[\"OPENAI_API_KEY\"] = openai_api_key"]},{"cell_type":"markdown","metadata":{"id":"KvGzAM9ii5lp"},"source":["## Prep truthful_qa dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vQjxWCHQjCnx"},"outputs":[],"source":["from datasets import load_dataset\n","\n","ds = load_dataset(\"truthfulqa/truthful_qa\", \"generation\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YicCCwMIjysw"},"outputs":[],"source":["\n","# Preview the first 5 examples from the training split\n","print(ds['validation'][:5])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"62MaOmrckPJp"},"outputs":[],"source":["# Export the 'train' split to a JSON file named 'commonsense_qa_train.json'\n","ds['validation'].to_json(\"truthful_qa_validation.json\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UjotODGmoH08"},"outputs":[],"source":["import json\n","\n","# load json file\n","with open(\"generated_answers_20250126_161052.json\", \"r\") as file:\n","    data = file.readlines()\n","\n","# extract questions and best answers\n","result = {}\n","for line in data:\n","    item = json.loads(line)\n","    # too many 'I have no comment' answers. Let's get rid of them\n","    if item[\"best_answer\"] != \"I have no comment\":\n","        result[item[\"question\"]] = item[\"best_answer\"]\n","\n","# save the simplified format\n","with open(\"truthful_qa_validation_large.json\", \"w\") as file:\n","    json.dump(result, file, indent=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K-gPi0zkqF7O"},"outputs":[],"source":["# might want to try other qa sets as well, so let's universalise\n","!cp truthful_qa_validation_reduced.json qa_data.json"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T7YXb-hlrE7A"},"outputs":[],"source":["with open(\"qa_data.json\", \"r\") as file:\n","    qa_data = json.load(file)"]},{"cell_type":"markdown","metadata":{"id":"6d92f64e"},"source":["## Camel-AI chat agent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-MVN2A0ZeFCr"},"outputs":[],"source":["sys_msg = 'You are a genius at slow-thinking data and code'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NWiLRs12jhh0"},"outputs":[],"source":["from camel.models import ModelFactory\n","from camel.types import ModelPlatformType, ModelType\n","from camel.configs import ChatGPTConfig\n","\n","# import userdata and instantiate openai key\n","from google.colab import userdata\n","OPENAI_API_KEY = userdata.get('openai')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nlOh4CO5jhh0"},"outputs":[],"source":["# Define the model\n","model = ModelFactory.create(\n","    model_platform=ModelPlatformType.OPENAI,\n","    model_type=ModelType.GPT_4O_MINI,\n","    model_config_dict=ChatGPTConfig().as_dict(), # [Optional] the config for model\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"43WlRKdtjhh0"},"outputs":[],"source":["from camel.agents import ChatAgent\n","chat_agent = ChatAgent(\n","    system_message=sys_msg,\n","    model=model,\n","    message_window_size=10,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2IzPHXIsjvQc"},"outputs":[],"source":["# Create an instance of CoTDataGenerator\n","testo1 = CoTDataGenerator(chat_agent, golden_answers=qa_data)\n","# I added  \"do not exceed 200 words in your answer\" to the prompt.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SVzVLXdMjvQd"},"outputs":[],"source":["# Record generated answers\n","generated_answers = {}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-xusWtekemx"},"outputs":[],"source":["# Test Q&A\n","for question in list(qa_data.keys())[:100]: # Convert dict_keys to a list for slicing purposes\n","    print(f\"Question: {question}\")\n","\n","    # Get AI's thought process and answer\n","    answer = testo1.get_answer(question)\n","    generated_answers[question] = answer\n","    print(f\"AI's thought process and answer:\\n{answer}\")\n","\n","    # Verify the answer\n","    is_correct = testo1.verify_answer(question, answer)\n","    print(f\"Answer verification result: {'Correct' if is_correct else 'Incorrect'}\")\n","    print(\"-\" * 50)\n","    print()  # Add a new line at the end of each iteration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wasTmIB2kemx"},"outputs":[],"source":["# export the generated answers to a JSON file and transform these to Alpaca traing data format\n","\n","simplified_output = {\n","    'timestamp': datetime.now().isoformat(),\n","    'qa_pairs': generated_answers\n","}\n","simplified_file = f'generated_answers_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.json'\n","with open(simplified_file, 'w', encoding='utf-8') as f:\n","    json.dump(simplified_output, f, ensure_ascii=False, indent=2)\n","print(f\"The generated answers have been exported to: {simplified_file}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LwnoMsfAkemy"},"outputs":[],"source":["import json\n","from datetime import datetime\n","\n","def transform_qa_format(input_file):\n","    # Read the input JSON file\n","    with open(input_file, 'r', encoding='utf-8') as f:\n","        data = json.load(f)\n","\n","    # Transform the data\n","    transformed_data = []\n","    for question, answer in data['qa_pairs'].items():\n","        transformed_pair = {\n","            \"instruction\": question,\n","            \"input\": \"\",\n","            \"output\": answer\n","        }\n","        transformed_data.append(transformed_pair)\n","\n","    # Generate output filename with timestamp\n","    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","    output_file = f'transformed_qa_{timestamp}.json'\n","\n","    # Write the transformed data\n","    with open(output_file, 'w', encoding='utf-8') as f:\n","        json.dump(transformed_data, f, ensure_ascii=False, indent=2)\n","\n","    return output_file, transformed_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gr3Tg_rMkemy"},"outputs":[],"source":["simplified_file = f'/content/drive/MyDrive/git/nlp_module_4_QA/generated_answers_20250126_161052.json'\n","output_file, transformed_data = transform_qa_format(simplified_file)\n","print(f\"Transformation complete. Output saved to: {output_file}\")"]},{"cell_type":"markdown","metadata":{"id":"PCICphUnGnpk"},"source":["\n","# Model prep"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BBgtDHlGHVLA"},"outputs":[],"source":["# install unsloth & nightly\n","!pip install unsloth\n","# nightly (idk if needed)\n","!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"]},{"cell_type":"markdown","metadata":{"id":"vzoDKg6NGnp6"},"source":["## choose the base model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s9c85bVpHGwX"},"outputs":[],"source":["# install flash attention (apparently faster?)\n","%%capture\n","!pip install --no-deps --upgrade \"flash-attn>=2.6.3\"\n","!pip install bitsandbytes"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":247,"referenced_widgets":["380c41e3693542278fa8245110a42ebc","974d697eedae4444986e73ac69dd7a8c","faadce184c61453ab92aec3065868586","c1cf6ab6c1484628997a1e699bdd95e3","00310ba744fd49b2aabd12a9c68a9773","91c066ed708341b1a760a1a812b3a570","cd73582d59d6484d9c2ba0bf5f110beb","b5b88da9a7b0490a8d1ceb06b3c4cc93","401d61e93cc4458f9e23cf5a810eafe4","ce8f3b67e855430f824f870fd2c38270","e63b3f60d4944674b2af407b6d937c6a","f4902404334940c0b93387979500fd3d","7714a06109db46649c309a3bcc7059ae","818fb67113f643a785f2105a98449b4f","619ac32b8f7b404b82d9ccccfced8623","6e427de84b334594ad80d0511f104d10","69e1a6255d0f49f5b6c677e9969bd217","15862df634d64f879cef426083ef0fc0","ba84eeecdb624f8fb9b57452bb3e66f7","d3270dfe8b2a401fb1900db737ec8157","e673f3dbabce42c3b7ed5bf82234a520","71c135b62d924c7f989d8607585312de","c5c6261bb7c34435b1dd585f3dbb8119","ade45f9a34984d1ba783e81a8fca8a77","07e9fc56b53741cc93ca0795a44ad35c","0b647496d45d43968e571ab0e12c0003","4d9984a9d8aa4164909d52ebb5d9cebb","09f19732c409478ba504faaa9cefdb90","0f70f8181fa04b8291142ff6d42aea2a","649fd3d18cb046fbbbb918b4b429dd80","f3db50ceebee4d24b5c5467a36e04de2","45ac9c664709415ba0c8ec8e49ccc3ec","1afef14db7cb4e1fb3da17fc6109d9c1","f40d586e9ecd4a63adfcde4bb5829349","7f93f148124a4c789c586d6e4fe90b5b","5b307d13d90c4e418938693ab08b1da1","02df424a16124b25bca199eb4871b52c","144abadbc49a497fb07061361ad188b5","1c3fb46ea74d49d68de4a68fecb53bba","52e43b1829ec438387117577304b1198","f7a15d8c3a4d4f9c87698cf8d1154aa3","1bc330110c99415f8f5c5362eff6ef8a","afa7d7b87d89482dbcde0a10d6c751ba","b56eee383aa94ba48e6c7ff3c67c4d76"]},"executionInfo":{"elapsed":24975,"status":"ok","timestamp":1738051296593,"user":{"displayName":"Bryce Maynard","userId":"16855679042583479157"},"user_tz":-60},"id":"ELtPavRgGnp6","outputId":"8ade9e41-f39f-40ec-b4d1-9fce6979caa0"},"outputs":[{"output_type":"stream","name":"stdout","text":["==((====))==  Unsloth 2025.1.7: Fast Gemma2 patching. Transformers: 4.47.1.\n","   \\\\   /|    GPU: Tesla T4. Max memory: 14.748 GB. Platform: Linux.\n","O^O/ \\_/ \\    Torch: 2.5.1+cu121. CUDA: 7.5. CUDA Toolkit: 12.1. Triton: 3.1.0\n","\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post1. FA2 = False]\n"," \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n","Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"380c41e3693542278fa8245110a42ebc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4902404334940c0b93387979500fd3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5c6261bb7c34435b1dd585f3dbb8119"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f40d586e9ecd4a63adfcde4bb5829349"}},"metadata":{}}],"source":["from unsloth import FastLanguageModel\n","#from datasets import datasets\n","import torch\n","max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n","dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n","load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n","\n","# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n","fourbit_models = [\n","    \"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",      # Llama-3.1 15 trillion tokens model 2x faster!\n","    \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n","    \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n","    \"unsloth/Meta-Llama-3.1-405B-bnb-4bit\",    # We also uploaded 4bit for 405b!\n","    \"unsloth/Mistral-Nemo-Base-2407-bnb-4bit\", # New Mistral 12b 2x faster!\n","    \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\",\n","    \"unsloth/mistral-7b-v0.3-bnb-4bit\",        # Mistral v3 2x faster!\n","    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n","    \"unsloth/Phi-3.5-mini-instruct\",           # Phi-3.5 2x faster!\n","    \"unsloth/Phi-3-medium-4k-instruct\",\n","    \"unsloth/gemma-2-9b-bnb-4bit\",\n","    \"unsloth/gemma-2-27b-bnb-4bit\",            # Gemma 2x faster!\n","] # More models at https://huggingface.co/unsloth\n","\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    #model_name = \"/content/drive/MyDrive/models/gemma-2-2b_4bit\",\n","    model_name = \"google/gemma-2-2b\",\n","    max_seq_length = max_seq_length,\n","    dtype = dtype,\n","    load_in_4bit = load_in_4bit,\n","    token = HUGGINGFACE_TOKEN,\n",")"]},{"cell_type":"markdown","metadata":{"id":"9oh9i15uGnp-"},"source":["## add LoRA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LAKez2sLGnp_"},"outputs":[],"source":["model = FastLanguageModel.get_peft_model(\n","    model,\n","    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n","    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n","                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n","    lora_alpha = 16,\n","    lora_dropout = 0, # Supports any, but = 0 is optimized\n","    bias = \"none\",    # Supports any, but = \"none\" is optimized\n","    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n","    random_state = 3407,\n","    use_rslora = False,  # We support rank stabilized LoRA\n","    loftq_config = None, # And LoftQ\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tgUgqkTqS-lo"},"outputs":[],"source":["print(model)"]},{"cell_type":"markdown","metadata":{"id":"1E3MccBgGnp_"},"source":["### Convert CoT data into an SFT-compliant training data format"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vvsm9YGIpCBM"},"outputs":[],"source":["from functools import partial\n","from transformers import DataCollatorForLanguageModeling\n","from datasets import load_dataset\n","\n","# 2. Define the alpaca prompt format and EOS_TOKEN\n","alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n","\n","### Instruction:\n","{}\n","\n","### Input:\n","{}\n","\n","### Response:\n","{}\"\"\"\n","\n","EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n","\n","\n","# 3. Define formatting function\n","def formatting_prompts_func(examples):\n","    instructions = examples[\"instruction\"]\n","    inputs       = examples[\"input\"]\n","    outputs      = examples[\"output\"]\n","    texts = []\n","    for instruction, input, output in zip(instructions, inputs, outputs):\n","        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n","        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n","        texts.append(text)\n","    return { \"text\" : texts, }\n","\n","\n","# 4. Define tokenize function\n","def tokenize_function(tokenizer, max_seq_length, element):\n","    outputs = tokenizer(\n","        element[\"text\"],\n","        add_special_tokens=True,\n","        truncation=True,\n","        padding=False,\n","        max_length=max_seq_length,\n","        return_overflowing_tokens=False,\n","        return_length=False,\n","    )\n","    return {\"input_ids\": outputs[\"input_ids\"], \"attention_mask\": outputs[\"attention_mask\"]}\n","\n","\n","# 5. Tokenize function using partial to pass tokenizer and max_seq_length\n","tokenize = partial(tokenize_function, tokenizer, max_seq_length)\n","\n","# 6. Load and prepare datasets\n","# calculate the split index for 80% of the data\n","total_samples = len(load_dataset(\"0fg/qa-dataset-20250126_large\", split=\"train\"))  # Get the total number of samples\n","split_index = int(total_samples * 0.8)\n","\n","# Load the datasets with correct split syntax\n","dataset = load_dataset(\"0fg/qa-dataset-20250126_large\", split=f\"train[:{split_index}]\")  # 80% for training\n","val_dataset = load_dataset(\"0fg/qa-dataset-20250126_large\", split=f\"train[{split_index}:]\")  # 20% for validation\n","\n","\n","dataset = dataset.map(formatting_prompts_func, batched=True)\n","val_dataset = val_dataset.map(formatting_prompts_func, batched=True)\n","\n","\n","# 7. Tokenize the datasets\n","tokenized_dataset = dataset.map(tokenize, batched=True, num_proc=2, remove_columns=[\"text\"])\n","tokenized_val_dataset = val_dataset.map(tokenize, batched=True, num_proc=2, remove_columns=[\"text\"])\n","\n","\n","\n","# 8. Data collator\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2UqVzhMapnQW"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Qffwpm-kGnp_"},"source":["# training"]},{"cell_type":"markdown","metadata":{"id":"FdNKPRib-wmv"},"source":["## set up eval metrics"]},{"cell_type":"code","source":["!pip install evaluate"],"metadata":{"id":"8wrg0hxTcqgL"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aln5YNj--2C8"},"outputs":[],"source":["from datasets import load_dataset\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n","import numpy as np\n","import evaluate\n","from sklearn.metrics import f1_score\n","\n","f1_metric = evaluate.load(\"f1\")\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = predictions.argmax(axis=-1)  # Convert logits to predictions\n","    f1 = f1_score(labels, predictions, average=\"weighted\") # or another average\n","    return {\"f1\": f1}"]},{"cell_type":"markdown","metadata":{"id":"4lzimHAOoGWr"},"source":["## instantiate trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mp0DcpcznPAO"},"outputs":[],"source":["# let's try it with the normal Transformer trainer since SFTTrainer is weird...\n","from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n","from unsloth import is_bfloat16_supported\n","from functools import partial\n","\n","\n","# training arguments\n","training_args = TrainingArguments(\n","    per_device_train_batch_size = 2,\n","    gradient_accumulation_steps = 4,\n","    warmup_steps = 5,\n","    num_train_epochs = 1, # 1 is full training run.\n","    # max_steps = 100,\n","    learning_rate = 1e-4,\n","    fp16 = not is_bfloat16_supported(),\n","    bf16 = is_bfloat16_supported(),\n","    optim = \"adamw_8bit\",\n","    weight_decay = 0.01,\n","    lr_scheduler_type = \"linear\",\n","    seed = 3407,\n","    output_dir = \"outputs\",\n","    report_to = \"none\", # Use this for WandB etc\n","    eval_steps=2, # added to check whether it outputs val loss during training\n","    logging_steps=1, # added to check whether it outputs val loss during training\n","    eval_strategy=\"steps\",\n","    prediction_loss_only=False,\n",")\n","\n","\n","# instantiate the Trainer\n","trainer = Trainer(\n","    model = model,\n","    args = training_args,\n","    train_dataset = tokenized_dataset,\n","    eval_dataset = tokenized_val_dataset,\n","    # compute_metrics=compute_metrics,\n","    data_collator = data_collator,\n","\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ff23st_bqeCd"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"a7Ii6bjlaY7g"},"source":["## Start model training"]},{"cell_type":"markdown","metadata":{"id":"47c6mYoDiJOp"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jR0IPGI2nIoD"},"outputs":[],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OsF42Au8iKN5"},"outputs":[],"source":["# check that everything is running on GPU\n","import torch\n","print(torch.cuda.is_available())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f9RSzOXLhcUN"},"outputs":[],"source":["!pip install unsloth\n","!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RTuNG4H2ahve"},"outputs":[],"source":["trainer_stats = trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"df2f8202"},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BQ7pEtzR3OkV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738051595765,"user_tz":-60,"elapsed":248522,"user":{"displayName":"Bryce Maynard","userId":"16855679042583479157"}},"outputId":"5d2e032a-87b4-4255-9962-ee522c10c881"},"outputs":[{"output_type":"stream","name":"stdout","text":["How many R in Strawberry?\n","\n","[User 0001]\n","\n","I'm looking for a good strain that is a good hybrid, and has a good high. I'm not looking for a sativa, but I'm not looking for a indica either. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa. I'm looking for a good hybrid. I'm not looking for a strain that is a 100% indica or 100% sativa.\n"]}],"source":["# alpaca_prompt is copied from above\n","\n","FastLanguageModel.for_inference(model)  # Enable native 2x faster inference\n","test_prompt = \"How many R in Strawberry?\"\n","# Prepare the input for inference\n","inputs = tokenizer(\n","    # [\n","    #     alpaca_prompt.format(\n","    #         \"how many r in strawberry？\",  # Instruction\n","    #         \"\",  # Input (empty for this example)\n","    #         \"\",  # Output (leave blank for generation)\n","    #     )\n","    # ],\n","    test_prompt,\n","    return_tensors=\"pt\"\n",").to(\"cuda\")\n","\n","# Generate the output\n","outputs = model.generate(\n","    **inputs,\n","    max_new_tokens=4096,  # Maximum number of tokens to generate\n","    use_cache=True        # Use cache for faster inference\n",")\n","\n","# Decode the generated output and clean it\n","decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","\n","# Print the cleaned output\n","print(decoded_outputs[0])  # Print the first (and only) output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JByUkCipjuuU"},"outputs":[],"source":["print(model_name)"]},{"cell_type":"markdown","metadata":{"id":"7d8fbdbe"},"source":["## Saving and Loading finetuned models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"53b39972"},"outputs":[],"source":["model.save_pretrained(\"/content/drive/MyDrive/models/LoRA/gemma-2-2b_4bit_LoRA_v4\") # Local saving\n","tokenizer.save_pretrained(\"/content/drive/MyDrive/models/LoRA/gemma-2-2b_4bit_LoRA_v4\")"]},{"cell_type":"markdown","metadata":{"id":"d44ea806"},"source":["Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9150c9e3"},"outputs":[],"source":["if True:\n","    from unsloth import FastLanguageModel\n","    model, tokenizer = FastLanguageModel.from_pretrained(\n","        model_name = \"/content/drive/MyDrive/models/LoRA/gemma-2-2b_4bit_LoRA_v4\", # model used for traingin\n","        max_seq_length = max_seq_length,\n","        dtype = dtype,\n","        load_in_4bit = load_in_4bit,\n","    )\n","    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n","\n","# alpaca_prompt copied from above\n","\n","inputs = tokenizer(\n","[\n","    alpaca_prompt.format(\n","        \"How many r in strawberry？\", # instruction\n","        \"\", # input\n","        \"\", # output - leave this blank for generation!\n","    )\n","], return_tensors = \"pt\").to(\"cuda\")\n","\n","from transformers import TextStreamer\n","text_streamer = TextStreamer(tokenizer)\n","_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 4098)"]},{"cell_type":"markdown","metadata":{"id":"0QswR8AXQAFg"},"source":["# log training runs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fb3jTBfMQGvl"},"outputs":[],"source":["from datetime import datetime\n","import csv\n","\n","# create logging function\n","def log_training_details(trainer, model_name, parameters):\n","    \"\"\"Logs training details to a CSV file.\n","    Args:\n","        trainer: The SFTTrainer instance.\n","        model_name: The name of the model.\n","        parameters: A dictionary containing the training parameters.\n","    \"\"\"\n","    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","\n","    # error handling for training loss\n","    try:\n","        training_loss = trainer.state.log_history[-1][\"loss\"]  # Get the last loss value\n","    except (KeyError, IndexError):\n","        training_loss = None  # handle cases where \"loss\" key is missing\n","        print(\"Warning: 'loss' key not found in training log. Setting loss to None.\")\n","\n","    with open(\"/content/drive/MyDrive/git/nlp_module_4_QA/training_log.csv\", \"a\", newline=\"\") as csvfile:\n","        writer = csv.writer(csvfile)\n","        # write header if file is empty\n","        if csvfile.tell() == 0:\n","            writer.writerow([\"Timestamp\", \"Model Name\", \"Training Loss\"] + list(parameters.keys()))\n","\n","        writer.writerow([timestamp, model_name, training_loss] + list(parameters.values()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MsRjrvmYYMhf"},"outputs":[],"source":["# call logging function\n","\n","from datetime import datetime\n","import csv\n","\n","parameters = {\n","    \"per_device_train_batch_size\": trainer.args.per_device_train_batch_size,\n","    \"gradient_accumulation_steps\": trainer.args.gradient_accumulation_steps,\n","    \"warmup_steps\": trainer.args.warmup_steps,\n","    \"max_steps\": trainer.args.max_steps,\n","    \"learning_rate\": trainer.args.learning_rate,\n","    \"optim\": trainer.args.optim,\n","    \"lr_scheduler_type\": trainer.args.lr_scheduler_type,\n","    \"num_train_epochs\": trainer.args.num_train_epochs,\n","}\n","log_training_details(trainer, \"gemma-2-2b_4bit_LoRA_v1\", parameters)"]},{"cell_type":"markdown","metadata":{"id":"IJfBdaOfWblE"},"source":["# Evaluation & Benchmarks"]},{"cell_type":"markdown","metadata":{"id":"gkiDoDG3nAnW"},"source":["## Plots"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LWgnkhIvppC-"},"outputs":[],"source":["import IPython\n","\n","app = IPython.Application.instance()\n","app.kernel.do_shutdown(True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-d5WXJ3K02i-"},"outputs":[],"source":["log_history = trainer.state.log_history\n","print(log_history)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ep5r-BaEsMz"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","loss_values = [entry['loss'] for entry in log_history if 'loss' in entry]\n","train_steps = [entry['step'] for entry in log_history if 'loss' in entry]\n","\n","# extract the evaluation loss and steps\n","eval_loss_values = [entry['eval_loss'] for entry in log_history if 'eval_loss' in entry]\n","eval_steps = [entry['step'] for entry in log_history if 'eval_loss' in entry]\n","\n","plt.plot(train_steps, loss_values, label='Training Loss')\n","plt.plot(eval_steps, eval_loss_values, label='Evaluation Loss')\n","\n","plt.xlabel('Training Step')\n","plt.ylabel('Loss')\n","plt.title('Training and Evaluation Loss over Time')\n","plt.legend()\n","\n","plt.ylim(0, 1.75)  #\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"tgEGbDgGm4LN"},"source":["## human evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IvXnt8I1m65Q"},"outputs":[],"source":["!pip install unsloth\n","!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n","from unsloth import FastLanguageModel\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# tokenizer = AutoTokenizer.from_pretrained(\"unsloth/gemma-2b-bnb-4bit\")\n","# model = AutoModelForCausalLM.from_pretrained(\"unsloth/gemma-2b-bnb-4bit\")\n","\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name=\"google/gemma-2b\",\n","    max_seq_length=2048,\n","    dtype= None,\n","    load_in_4bit=True,\n","    token = HUGGINGFACE_TOKEN,\n",")\n","\n","FastLanguageModel.for_inference(model)\n","\n","import json\n","\n","with open(\"/content/drive/MyDrive/git/human_eval_questions.json\", \"r\") as f:\n","    questions = json.load(f)\n","\n","import csv\n","\n","qa_pairs = []\n","for question_data in questions:\n","    question = question_data[\"instruction\"]\n","\n","    inputs = tokenizer(question, return_tensors=\"pt\").to(\"cuda\")\n","    outputs = model.generate(**inputs, max_new_tokens=512)\n","    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","\n","    qa_pairs.append([question, answer])\n","\n","with open(\"/content/drive/MyDrive/githuman_eval_answers_gemma.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n","    writer = csv.writer(csvfile)\n","    writer.writerow([\"Question\", \"Answer\"])\n","    writer.writerows(qa_pairs)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jqokN5-4nnLN"},"source":["## MMLU"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JzB3E-lx946Z"},"outputs":[],"source":["#!pip install bitsandbytes\n","!pip install -U bitsandbytes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"prp2bbQh9ptp"},"outputs":[],"source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","from deepeval.models.base_model import DeepEvalBaseLLM\n","from typing import Any\n","from deepeval.types import LLMResponse\n","\n","class Gemma22bLoRA(DeepEvalBaseLLM):\n","    def __init__(self, model_path):\n","        self.model_path = model_path\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n","        self.model = AutoModelForCausalLM.from_pretrained(model_path)\n","\n","    def load_model(self):\n","        return self.model\n","\n","    def generate(self, prompt: str, schema: Any = None) -> LLMResponse:\n","        model = self.load_model()\n","        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        model_inputs = self.tokenizer([prompt], return_tensors=\"pt\").to(device)\n","        model.to(device)\n","\n","        generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n","        generated_text = self.tokenizer.batch_decode(generated_ids)[0]\n","\n","        return LLMResponse(\n","            text=generated_text,\n","            answer=generated_text\n","        )\n","\n","\n","    async def a_generate(self, prompt: str, schema: Any = None) -> LLMResponse: # add schema argument with default value\n","        return self.generate(prompt, schema)\n","\n","    def batch_generate(self, prompts: list[str]) -> list[str]:\n","        model = self.load_model()\n","        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        model_inputs = self.tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","        model.to(device)\n","\n","        generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n","        return self.tokenizer.batch_decode(generated_ids)\n","\n","    def get_model_name(self):\n","        return \"Gemma 2.2B 4bit LoRA v3\"\n","\n","# instantiate custom model class\n","model_path = '/content/drive/MyDrive/models/LoRA/gemma-2-2b_4bit_LoRA_v3'\n","gemma_lora = Gemma22bLoRA(model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TbKlmIccnnLP"},"outputs":[],"source":["!pip install deepeval"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QFZM2sGLnnLP"},"outputs":[],"source":["from deepeval.benchmarks import MMLU\n","from deepeval.benchmarks.mmlu.task import MMLUTask\n","\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from datasets import load_dataset\n","\n","# define benchmark with specific tasks and shots\n","benchmark = MMLU(\n","    tasks=[MMLUTask.FORMAL_LOGIC],\n","    n_shots=3\n",")\n","\n","# instantiate  custom model class\n","model_path = '/content/drive/MyDrive/models/LoRA/gemma-2-2b_4bit_LoRA_v3'\n","gemma_lora = Gemma22bLoRA(model_path)  # create an instance of Gemma22bLoRA\n","\n","\n","benchmark.evaluate(model=gemma_lora)  #use the instance, not the class\n","print(benchmark.overall_score)"]}],"metadata":{"colab":{"collapsed_sections":["kQORzf4vXn3p","gkiDoDG3nAnW"],"provenance":[],"mount_file_id":"1BxgTZDpmde3bTidrJRu8Py75NBJGdAH5","authorship_tag":"ABX9TyPMV9WQki6kwIQMVg7bvZnj"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"380c41e3693542278fa8245110a42ebc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_974d697eedae4444986e73ac69dd7a8c","IPY_MODEL_faadce184c61453ab92aec3065868586","IPY_MODEL_c1cf6ab6c1484628997a1e699bdd95e3"],"layout":"IPY_MODEL_00310ba744fd49b2aabd12a9c68a9773"}},"974d697eedae4444986e73ac69dd7a8c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91c066ed708341b1a760a1a812b3a570","placeholder":"​","style":"IPY_MODEL_cd73582d59d6484d9c2ba0bf5f110beb","value":"tokenizer_config.json: 100%"}},"faadce184c61453ab92aec3065868586":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5b88da9a7b0490a8d1ceb06b3c4cc93","max":46405,"min":0,"orientation":"horizontal","style":"IPY_MODEL_401d61e93cc4458f9e23cf5a810eafe4","value":46405}},"c1cf6ab6c1484628997a1e699bdd95e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce8f3b67e855430f824f870fd2c38270","placeholder":"​","style":"IPY_MODEL_e63b3f60d4944674b2af407b6d937c6a","value":" 46.4k/46.4k [00:00&lt;00:00, 1.35MB/s]"}},"00310ba744fd49b2aabd12a9c68a9773":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91c066ed708341b1a760a1a812b3a570":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd73582d59d6484d9c2ba0bf5f110beb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5b88da9a7b0490a8d1ceb06b3c4cc93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"401d61e93cc4458f9e23cf5a810eafe4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ce8f3b67e855430f824f870fd2c38270":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e63b3f60d4944674b2af407b6d937c6a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4902404334940c0b93387979500fd3d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7714a06109db46649c309a3bcc7059ae","IPY_MODEL_818fb67113f643a785f2105a98449b4f","IPY_MODEL_619ac32b8f7b404b82d9ccccfced8623"],"layout":"IPY_MODEL_6e427de84b334594ad80d0511f104d10"}},"7714a06109db46649c309a3bcc7059ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_69e1a6255d0f49f5b6c677e9969bd217","placeholder":"​","style":"IPY_MODEL_15862df634d64f879cef426083ef0fc0","value":"tokenizer.model: 100%"}},"818fb67113f643a785f2105a98449b4f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba84eeecdb624f8fb9b57452bb3e66f7","max":4241003,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d3270dfe8b2a401fb1900db737ec8157","value":4241003}},"619ac32b8f7b404b82d9ccccfced8623":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e673f3dbabce42c3b7ed5bf82234a520","placeholder":"​","style":"IPY_MODEL_71c135b62d924c7f989d8607585312de","value":" 4.24M/4.24M [00:00&lt;00:00, 11.2MB/s]"}},"6e427de84b334594ad80d0511f104d10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69e1a6255d0f49f5b6c677e9969bd217":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15862df634d64f879cef426083ef0fc0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ba84eeecdb624f8fb9b57452bb3e66f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3270dfe8b2a401fb1900db737ec8157":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e673f3dbabce42c3b7ed5bf82234a520":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"71c135b62d924c7f989d8607585312de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c5c6261bb7c34435b1dd585f3dbb8119":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ade45f9a34984d1ba783e81a8fca8a77","IPY_MODEL_07e9fc56b53741cc93ca0795a44ad35c","IPY_MODEL_0b647496d45d43968e571ab0e12c0003"],"layout":"IPY_MODEL_4d9984a9d8aa4164909d52ebb5d9cebb"}},"ade45f9a34984d1ba783e81a8fca8a77":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_09f19732c409478ba504faaa9cefdb90","placeholder":"​","style":"IPY_MODEL_0f70f8181fa04b8291142ff6d42aea2a","value":"special_tokens_map.json: 100%"}},"07e9fc56b53741cc93ca0795a44ad35c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_649fd3d18cb046fbbbb918b4b429dd80","max":636,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f3db50ceebee4d24b5c5467a36e04de2","value":636}},"0b647496d45d43968e571ab0e12c0003":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_45ac9c664709415ba0c8ec8e49ccc3ec","placeholder":"​","style":"IPY_MODEL_1afef14db7cb4e1fb3da17fc6109d9c1","value":" 636/636 [00:00&lt;00:00, 15.0kB/s]"}},"4d9984a9d8aa4164909d52ebb5d9cebb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09f19732c409478ba504faaa9cefdb90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f70f8181fa04b8291142ff6d42aea2a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"649fd3d18cb046fbbbb918b4b429dd80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f3db50ceebee4d24b5c5467a36e04de2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"45ac9c664709415ba0c8ec8e49ccc3ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1afef14db7cb4e1fb3da17fc6109d9c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f40d586e9ecd4a63adfcde4bb5829349":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7f93f148124a4c789c586d6e4fe90b5b","IPY_MODEL_5b307d13d90c4e418938693ab08b1da1","IPY_MODEL_02df424a16124b25bca199eb4871b52c"],"layout":"IPY_MODEL_144abadbc49a497fb07061361ad188b5"}},"7f93f148124a4c789c586d6e4fe90b5b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c3fb46ea74d49d68de4a68fecb53bba","placeholder":"​","style":"IPY_MODEL_52e43b1829ec438387117577304b1198","value":"tokenizer.json: 100%"}},"5b307d13d90c4e418938693ab08b1da1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7a15d8c3a4d4f9c87698cf8d1154aa3","max":17525357,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1bc330110c99415f8f5c5362eff6ef8a","value":17525357}},"02df424a16124b25bca199eb4871b52c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_afa7d7b87d89482dbcde0a10d6c751ba","placeholder":"​","style":"IPY_MODEL_b56eee383aa94ba48e6c7ff3c67c4d76","value":" 17.5M/17.5M [00:00&lt;00:00, 45.1MB/s]"}},"144abadbc49a497fb07061361ad188b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c3fb46ea74d49d68de4a68fecb53bba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52e43b1829ec438387117577304b1198":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7a15d8c3a4d4f9c87698cf8d1154aa3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1bc330110c99415f8f5c5362eff6ef8a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"afa7d7b87d89482dbcde0a10d6c751ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b56eee383aa94ba48e6c7ff3c67c4d76":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}