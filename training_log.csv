Model Name,per_device_train_batch_size,gradient_accumulation_steps,warmup_steps,max_steps,learning_rate,optim,lr_scheduler_type,weight decay,LoRA rank,LoRA dropout,num_train_epochs,dataset size,training loss,validation loss,
gemma-2-2b_4bit_LoRA_v1,2,4,5,60,0.0002,adamw_8bit,linear,,16,n/a,3,100,0.1513,1.552277,